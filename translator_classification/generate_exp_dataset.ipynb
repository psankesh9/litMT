{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Experiment Datasets for Translator Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having aligned our book source and translated text and implemented paragraph sorting/filtering based on semantic similarity, we can now generate the train/val/test datasets for our experiments.\n",
    "\n",
    "In our experiments we want to compare various input/data settings:\n",
    "1. Filtered vs unfiltered paragraph alignments - we hypothesize that removing poor alignments will improve translator classification performance.\n",
    "2. Multilingual-BERT Tokenization - we explore if passing the Russian source with its English translation improves performances compared to only passing just the English translated paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIM Score Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having performed alignment evaluation based on a number of different semantic similarity metrics, we chose to use the SIM similarity metric, as did the par3 paper which created the paragraph alignment method we used.\n",
    "\n",
    "Paper: SIM score metric from [Beyond BLEU: Training Neural Machine Translation with Semantic Similarity](https://arxiv.org/pdf/1909.06694)\n",
    "\n",
    "Implementation from [https://github.com/katherinethai/par3](https://github.com/katherinethai/par3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the SIM score metric, load in the sim models files from the par3 directory you cloned: \\\n",
    "`%load par3/par3_align/similarity/sim_models.py`\\\n",
    "`%load par3/par3_align/similarity/sim_utils.py`\\\n",
    "`%load par3/par3_align/similarity/test_sim.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load par3/par3_align/similarity/sim_models.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.distance import CosineSimilarity\n",
    "import numpy as np\n",
    "\n",
    "class ParaModel(nn.Module):\n",
    "\n",
    "    def __init__(self, args, vocab):\n",
    "        super(ParaModel, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab = vocab\n",
    "        self.gpu = args.gpu\n",
    "\n",
    "        self.cosine = CosineSimilarity()\n",
    "\n",
    "    def compute_mask(self, lengths):\n",
    "\n",
    "        lengths = lengths.cpu()\n",
    "        max_len = torch.max(lengths)\n",
    "        range_row = torch.arange(0, max_len).long()[None, :].expand(lengths.size()[0], max_len)\n",
    "        mask = lengths[:, None].expand_as(range_row)\n",
    "        mask = range_row < mask\n",
    "        mask = mask.float()\n",
    "        if self.gpu >= 0:\n",
    "            mask = mask.cuda()\n",
    "        return mask\n",
    "\n",
    "    def torchify_batch(self, batch):\n",
    "\n",
    "        max_len = 0\n",
    "        for i in batch:\n",
    "            if len(i.embeddings) > max_len:\n",
    "                max_len = len(i.embeddings)\n",
    "\n",
    "        batch_len = len(batch)\n",
    "\n",
    "        np_sents = np.zeros((batch_len, max_len), dtype='int32')\n",
    "        np_lens = np.zeros((batch_len,), dtype='int32')\n",
    "\n",
    "        for i, ex in enumerate(batch):\n",
    "            np_sents[i, :len(ex.embeddings)] = ex.embeddings\n",
    "            np_lens[i] = len(ex.embeddings)\n",
    "\n",
    "        idxs, lengths, masks = torch.from_numpy(np_sents).long(), \\\n",
    "                               torch.from_numpy(np_lens).float().long(), \\\n",
    "                               self.compute_mask(torch.from_numpy(np_lens).long())\n",
    "\n",
    "        if self.gpu >= 0:\n",
    "            idxs = idxs.cuda()\n",
    "            lengths = lengths.cuda()\n",
    "            masks = masks.cuda()\n",
    "    \n",
    "        return idxs, lengths, masks\n",
    "\n",
    "    def scoring_function(self, g_idxs1, g_mask1, g_lengths1, g_idxs2, g_mask2, g_lengths2):\n",
    "\n",
    "        g1 = self.encode(g_idxs1, g_mask1, g_lengths1)\n",
    "        g2 = self.encode(g_idxs2, g_mask2, g_lengths2)\n",
    "        return self.cosine(g1, g2)\n",
    "\n",
    "class WordAveraging(ParaModel):\n",
    "\n",
    "    def __init__(self, args, vocab):\n",
    "        super(WordAveraging, self).__init__(args, vocab)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.embedding = nn.Embedding(len(self.vocab), self.args.dim)\n",
    "\n",
    "        if args.gpu >= 0:\n",
    "           self.cuda()\n",
    "\n",
    "    def encode(self, idxs, mask, lengths):\n",
    "        word_embs = self.embedding(idxs)\n",
    "        word_embs = word_embs * mask[:, :, None]\n",
    "        g = word_embs.sum(dim=1) / lengths[:, None].float()\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load par3/par3_align/similarity/sim_utils.py\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_wordmap(textfile):\n",
    "    words={}\n",
    "    We = []\n",
    "    f = io.open(textfile, 'r', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    if len(lines[0].split()) == 2:\n",
    "        lines.pop(0)\n",
    "    ct = 0\n",
    "    for (n,i) in enumerate(lines):\n",
    "        word = i.split(' ', 1)[0]\n",
    "        vec = i.split(' ', 1)[1].split(' ')\n",
    "        j = 0\n",
    "        v = []\n",
    "        while j < len(vec):\n",
    "            v.append(float(vec[j]))\n",
    "            j += 1\n",
    "        words[word] = ct\n",
    "        ct += 1\n",
    "        We.append(v)\n",
    "    return words, np.array(We)\n",
    "\n",
    "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // minibatch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                    minibatch_start + minibatch_size])\n",
    "        minibatch_start += minibatch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return zip(range(len(minibatches)), minibatches)\n",
    "\n",
    "def max_pool(x, lengths, gpu):\n",
    "    out = torch.FloatTensor(x.size(0), x.size(2)).zero_()\n",
    "    if gpu >= 0:\n",
    "        out = out.cuda()\n",
    "    for i in range(len(lengths)):\n",
    "        out[i] = torch.max(x[i][0:lengths[i]], 0)[0]\n",
    "    return out\n",
    "\n",
    "def mean_pool(x, lengths, gpu):\n",
    "    out = torch.FloatTensor(x.size(0), x.size(2)).zero_()\n",
    "    if gpu >= 0:\n",
    "        out = out.cuda()\n",
    "    for i in range(len(lengths)):\n",
    "        out[i] = torch.mean(x[i][0:lengths[i]], 0)\n",
    "    return out\n",
    "\n",
    "def lookup(words, w):\n",
    "    w = w.lower()\n",
    "    if w in words:\n",
    "        return words[w]\n",
    "\n",
    "class Example(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence.strip().lower()\n",
    "        self.embeddings = []\n",
    "        self.representation = None\n",
    "\n",
    "    def populate_embeddings(self, words):\n",
    "        sentence = self.sentence.lower()\n",
    "        arr = sentence.split()\n",
    "        for i in arr:\n",
    "            emb = lookup(words, i)\n",
    "            if emb:\n",
    "                self.embeddings.append(emb)\n",
    "        if len(self.embeddings) == 0:\n",
    "            self.embeddings.append(words['UUUNKKK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load par3/par3_align/similarity/test_sim.py\n",
    "import torch\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "tok = TreebankWordTokenizer()\n",
    "\n",
    "model = torch.load('/home/kkatsy/par3/par3_align/similarity/sim/sim.pt')\n",
    "state_dict = model['state_dict']\n",
    "vocab_words = model['vocab_words']\n",
    "args = model['args']\n",
    "# turn off gpu\n",
    "model = WordAveraging(args, vocab_words)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/home/kkatsy/par3/par3_align/similarity/sim/sim.sp.30k.model')\n",
    "model.eval()\n",
    "\n",
    "def make_example(sentence, model):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = \" \".join(tok.tokenize(sentence))\n",
    "    sentence = sp.EncodeAsPieces(sentence)\n",
    "    wp1 = Example(\" \".join(sentence))\n",
    "    wp1.populate_embeddings(model.vocab)\n",
    "    return wp1\n",
    "\n",
    "def find_similarity(s1, s2):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        s2 = [make_example(x, model) for x in s2]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        wx2, wl2, wm2 = model.torchify_batch(s2)\n",
    "        BATCH_SIZE = 512\n",
    "        all_scores = []\n",
    "        for i in range(0, len(wx1), BATCH_SIZE):\n",
    "            scores = model.scoring_function(wx1[i:i + BATCH_SIZE], wm1[i:i + BATCH_SIZE], wl1[i:i + BATCH_SIZE],\n",
    "                                            wx2[i:i + BATCH_SIZE], wm2[i:i + BATCH_SIZE], wl2[i:i + BATCH_SIZE])\n",
    "            all_scores.extend([x.item() for x in scores])\n",
    "        return all_scores\n",
    "\n",
    "def find_similarity_matrix(s1, s2):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        s2 = [make_example(x, model) for x in s2]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        wx2, wl2, wm2 = model.torchify_batch(s2)\n",
    "\n",
    "        BATCH_SIZE = 2000\n",
    "        vecs1 = []\n",
    "        vecs2 = []\n",
    "        for i in range(0, len(wx1), BATCH_SIZE):\n",
    "            curr_vecs1 = model.encode(idxs=wx1[i:i + BATCH_SIZE],\n",
    "                                      mask=wm1[i:i + BATCH_SIZE],\n",
    "                                      lengths=wl1[i:i + BATCH_SIZE])\n",
    "            vecs1.append(curr_vecs1)\n",
    "        for i in range(0, len(wx2), BATCH_SIZE):\n",
    "            curr_vecs2 = model.encode(idxs=wx2[i:i + BATCH_SIZE],\n",
    "                                      mask=wm2[i:i + BATCH_SIZE],\n",
    "                                      lengths=wl2[i:i + BATCH_SIZE])\n",
    "            vecs2.append(curr_vecs2)\n",
    "        vecs1 = torch.cat(vecs1)\n",
    "        vecs2 = torch.cat(vecs2)\n",
    "        dot_product = torch.matmul(vecs1, vecs2.t())\n",
    "\n",
    "        vecs1_norm = torch.norm(vecs1, dim=1, keepdim=True)\n",
    "        vecs2_norm = torch.norm(vecs2, dim=1, keepdim=True)\n",
    "        norm_product = torch.matmul(vecs1_norm, vecs2_norm.t())\n",
    "    return torch.div(dot_product, norm_product)\n",
    "\n",
    "def encode_text(s1):\n",
    "    with torch.no_grad():\n",
    "        s1 = [make_example(x, model) for x in s1]\n",
    "        wx1, wl1, wm1 = model.torchify_batch(s1)\n",
    "        vecs1 = model.encode(idxs=wx1, mask=wm1, lengths=wl1)\n",
    "        return vecs1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordAveraging(\n",
       "  (cosine): CosineSimilarity()\n",
       "  (embedding): Embedding(65733, 300)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = TreebankWordTokenizer()\n",
    "\n",
    "model = torch.load('/home/kkatsy/par3/par3_align/similarity/sim/sim.pt')\n",
    "state_dict = model['state_dict']\n",
    "vocab_words = model['vocab_words']\n",
    "args = model['args']\n",
    "# turn off gpu\n",
    "model = WordAveraging(args, vocab_words)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('/home/kkatsy/par3/par3_align/similarity/sim/sim.sp.30k.model')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(refs, cands, metric='sim'):\n",
    "    return find_similarity(refs,cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in aligned paragraph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('aligned_paragraph_dataset.pickle', 'rb') as fp:\n",
    "  aligned_paragraph_dataset = pickle.load(fp)\n",
    "\n",
    "with open('source_paragraph_dataset.pickle', 'rb') as fp:\n",
    "  source_paragraph_dataset = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DeadSouls', 'FathersAndSons', 'PoorFolk', 'Demons', 'AnnaKarenina', 'NotesFromUnderground', 'TheBrothersKaramazov', 'TheIdiot', 'CrimeAndPunishment'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_paragraph_dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting + Filtering Aligned Paragraph Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to filter out certain paragraph alignments, depending on what experiments we are running. \n",
    "\n",
    "We can filter by:\n",
    "- choosing how many of the aligned paragraphs to keep with the highest semantic similarity scores\n",
    "- choosing what percent of the top scoring aligned paragraphs to drop to remove perfect or near-perfect matches in the different translations since it gives no info on translator style\n",
    "- choosing min and max len of the aligned paragraphs: some aligned \"paragraphs\" are shorter than a sentence or only one or a couple words; some paragraphs are too long to be handled by multilingual-BERT tokenization\n",
    "- choosing scale of diff in length between the source and translated paragraphs; translations should approximately correspond to the length of the original paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from statistics import mean\n",
    "from operator import itemgetter\n",
    "\n",
    "\"\"\"\n",
    "Sort + filter out aligned paragraphs based on the SIM metric.\n",
    "\n",
    "Args:\n",
    "        par_list: list of aligned par lists, [[par1_tr1, par1_tr2, par1_tr3], [par2_tr1, par2_tr2, par2_tr3], ...]\n",
    "        top_k: num of aligned pars to return with highest similarity scores post-filtering for length\n",
    "        drop_top: percent of aligned pars\n",
    "        min_len: drop aligned pars where pars are shorter than min length\n",
    "        max_len: drop aligned pars where pars are longer than max length\n",
    "        align_scale: max permitted diff in len between source par and translated par\n",
    "\n",
    "    Returns:\n",
    "        i2score: list of tuples, (aligned paragraph index, similarity score of paragraph translations) for all aligned paragraphs in list\n",
    "        top_k_scores: list of tuples, (aligned paragraph index, similarity score of paragraph translations) for top k aligned paragraphs in list post-filtering\n",
    "\"\"\"\n",
    "\n",
    "def get_best_alignments(par_list, source_par_list, top_k_percent, num_k, drop_top, metric, min_len, max_len, align_scale):\n",
    "\n",
    "    # dict -> score:par_set\n",
    "    # iter thru par_list, prune by length, get metric for set\n",
    "    keep_index_list = []\n",
    "    i2score = {}\n",
    "    for i in range(len(par_list)):\n",
    "        keep_index_list.append(i)\n",
    "        par_set = par_list[i]\n",
    "\n",
    "        max_par_len = len(max(par_set, key = len))\n",
    "        min_par_len = len(min(par_set, key = len))\n",
    "        source_len = len(source_par_list[i])\n",
    "\n",
    "        if (min_par_len >= min_len) and (max_par_len) <= max_len and not all(x==par_set[0] for x in par_set) and (max_par_len <= align_scale*source_len) and (min_par_len*align_scale >= source_len):\n",
    "\n",
    "            pairs = list(itertools.combinations(par_set, 2))\n",
    "            refs, cands = [], []\n",
    "            for s1, s2 in pairs:\n",
    "                refs.append(s1)\n",
    "                cands.append(s2)\n",
    "                \n",
    "            pair_scores = get_score(refs, cands, metric)\n",
    "\n",
    "            average_score = mean(pair_scores)\n",
    "            i2score[i] = average_score\n",
    "\n",
    "    # get top k par sets\n",
    "    num_pars = len(list(i2score))\n",
    "    top_k = int(top_k_percent * num_pars)\n",
    "    if top_k >= num_k:\n",
    "        top_k_scores = sorted(i2score.items(), key=itemgetter(1), reverse=True)[int(num_pars*drop_top):int(num_pars*drop_top) + num_k]\n",
    "    else:\n",
    "        top_k_scores = sorted(i2score.items(), key=itemgetter(1), reverse=True)[int(num_pars*drop_top):int(num_pars*drop_top) + top_k]\n",
    "    \n",
    "    i2score = sorted(i2score.items(), key=itemgetter(1), reverse=True)\n",
    "    return i2score, top_k_scores, keep_index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Filtering Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_paragraph_len = 20\n",
    "max_paragraph_len = 1000000000000\n",
    "top_k_percent = 0.9\n",
    "num_k = 50000\n",
    "drop_top = 0.02\n",
    "align_scale = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicate which books will be assigned to the holdout set and what specific books or translators you want to not be included in the train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout\n",
    "# NotesFromUnderground - Katz, PV, Garnett, Hogarth\n",
    "# PoorFolk - McDuff, Hogarth, Garnett\n",
    "# TheIdiot - Garnett, McDuff, PV\n",
    "# CrimeAndPunishment - Katz, McDuff, PV, Garnett\n",
    "\n",
    "holdout_books = ['TheIdiot', 'NotesFromUnderground']\n",
    "ignore_books = []\n",
    "ignore_translator = ['Hogarth']\n",
    "translator_to_pars = {}\n",
    "translator_to_pars_holdout = {}\n",
    "\n",
    "# for each book in train:\n",
    "for book in sorted(list(aligned_paragraph_dataset.keys())):\n",
    "    # get par list of aligned sentences, best k alignments\n",
    "    book_par_list = [list(aligned_paragraph_dataset[book][p].values()) for p in range(len(aligned_paragraph_dataset[book]))]\n",
    "    source_par_list = source_paragraph_dataset[book]\n",
    "\n",
    "    if book in holdout_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, 1.0, 5000, 0, 'sim', min_paragraph_len, max_paragraph_len, 100)\n",
    "    elif book not in ignore_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, top_k_percent, num_k, drop_top, 'sim', min_paragraph_len, max_paragraph_len, align_scale)\n",
    "    else:\n",
    "        top_k = []\n",
    "\n",
    "    for i, sim in top_k:\n",
    "        par_trans_dict = aligned_paragraph_dataset[book][i]\n",
    "        par_source = source_paragraph_dataset[book][i]\n",
    "\n",
    "        for translator, t in par_trans_dict.items():\n",
    "            if translator not in ignore_translator:\n",
    "                t = t.replace('\\\\\\'', '\\'')\n",
    "                datum_dict = {'source':par_source, 'translation': t, 'idx': i, 'book': book, 'sim': sim, 'translator': translator}\n",
    "\n",
    "                if translator not in translator_to_pars.keys():\n",
    "                    translator_to_pars[translator] = []\n",
    "                    translator_to_pars_holdout[translator] = []\n",
    "                    \n",
    "                if book in holdout_books:\n",
    "                    translator_to_pars_holdout[translator].append(datum_dict)\n",
    "                else:\n",
    "                    translator_to_pars[translator].append(datum_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Holdout and Non-Holdout Data by Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-filtering, we have a disbalanced distribution of paragraphs per translator. We need to make sure we include the same amount of data per translator.\n",
    "\n",
    "We know the translator Katz has the least data, so we randomly sample and reduce the number of paragraphs we keep for the rest of the translators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6931\n"
     ]
    }
   ],
   "source": [
    "min_len = len(translator_to_pars['Katz'])\n",
    "print(min_len)\n",
    "for t in translator_to_pars.keys():\n",
    "    keep = sorted(translator_to_pars[t], key=lambda d: d['sim'], reverse=True)[:min_len]\n",
    "    translator_to_pars[t] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "min_len_h = len(translator_to_pars_holdout['Katz'])\n",
    "print(min_len_h)\n",
    "for t in translator_to_pars_holdout.keys():\n",
    "    keep = sample(translator_to_pars_holdout[t], min_len_h) \n",
    "    translator_to_pars_holdout[t] = keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the paragraphs we've kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'Из лицея молодой человек в первые два года приезжал на вакацию. Во время поездки в Петербург Варвары Петровны и Степана Трофимовича он присутствовал иногда на литературных вечерах, бывавших у мамаши, слушал и наблюдал. Говорил мало и всё по-прежнему был тих и застенчив. К Степану Трофимовичу относился с прежним нежным вниманием, но уже как-то сдержаннее: о высоких предметах и о воспоминаниях прошлого видимо удалялся с ним заговаривать. Кончив курс, он, по желанию мамаши, поступил в военную службу и вскоре был зачислен в один из самых видных гвардейских кавалерийских полков. Показаться мамаше в мундире он не приехал и редко стал писать из Петербурга. Денег Варвара Петровна посылала ему не жалея, несмотря на то что после реформы доход с ее имений упал до того, что в первое время она и половины прежнего дохода не получала. У ней, впрочем, накоплен был долгою экономией некоторый, не совсем маленький капитал. Ее очень интересовали успехи сына в высшем петербургском обществе. Что не удалось ей, то удалось молодому офицеру, богатому и с надеждами. Он возобновил такие знакомства, о которых она и мечтать уже не могла, и везде был принят с большим удовольствием. Но очень скоро начали доходить к Варваре Петровне довольно странные слухи: молодой человек как-то безумно и вдруг закутил. Не то чтоб он играл или очень пил; рассказывали только о какой-то дикой разнузданности, о задавленных рысаками людях, о зверском поступке с одною дамой хорошего общества, с которою он был в связи, а потом оскорбил ее публично. Что-то даже слишком уж откровенно грязное было в этом деле. Прибавляли сверх того, что он какой-то бретер, привязывается и оскорбляет из удовольствия оскорбить. Варвара Петровна волновалась и тосковала. Степан Трофимович уверял ее, что это только первые, буйные порывы слишком богатой организации, что море уляжется и что всё это похоже на юность принца Гарри, кутившего с Фальстафом, Пойнсом и мистрис Квикли, описанную у Шекспира. Варвара Петровна на этот раз не крикнула: «Вздор, вздор!», как повадилась в последнее время покрикивать очень часто на Степана Трофимовича, а, напротив, очень прислушалась, велела растолковать себе подробнее, сама взяла Шекспира и с чрезвычайным вниманием прочла бессмертную хронику. Но хроника ее не успокоила, да и сходства она не так много нашла. Она лихорадочно ждала ответов на несколько своих писем. Ответы не замедлили; скоро было получено роковое известие, что принц Гарри имел почти разом две дуэли, кругом был виноват в обеих, убил одного из своих противников наповал, а другого искалечил и вследствие таковых деяний был отдан под суд. Дело кончилось разжалованием в солдаты, с лишением прав и ссылкой на службу в один из пехотных армейских полков, да и то еще по особенной милости.',\n",
       "  'translation': 'For the first two years the young man came home from the lycée for vacations. While Varvara Petrovna and Stepan Trofimovich were in Petersburg, he was sometimes present at his mother\\'s literary evenings, listening and observing. He spoke little, and was quiet and shy as before. He treated Stepan Trofimovich with the former tender attentiveness, but now somehow more reservedly: he obviously refrained from talking with him about lofty subjects or memories of the past. In accordance with his mama\\'s wish, after completing his studies he entered military service and was soon enrolled in one of the most distinguished regiments of the Horse Guard. He did not come to show himself to his mama in his uniform and now rarely wrote from Petersburg. Varvara Petrovna sent him money without stint, in spite of the fact that the income from her estates fell so much after the reform that at first she did not get even half of her former income. However, through long economy she had saved up a certain not exactly small sum. She was very interested in her son\\'s successes in Petersburg high society. The young officer, rich and with expectations, succeeded where she had not. He renewed acquaintances of which she could no longer even dream, and was received everywhere with great pleasure. But very soon rather strange rumors began to reach Varvara Petrovna: the young man, somehow madly and suddenly, started leading a wild life. Not that he gambled or drank too much; there was only talk of some savage unbridledness, of some people being run over by horses, of some beastly behavior towards a lady of good society with whom he had had a liaison and whom he afterwards publicly insulted. There was something even too frankly dirty about this affair. It was added, furthermore, that he was some sort of swashbuckler, that he picked on people and insulted them for the pleasure of it. Varvara Petrovna was worried and anguished. Stepan Trofimovich assured her that these were merely the first stormy impulses of an overabundant constitution, that the sea would grow calm, and that it all resembled Shakespeare\\'s description of the youth of Prince Harry, carousing with Falstaff, Poins, and Mistress Quickly. This time Varvara Petrovna did not shout \"Nonsense, nonsense!\" as it had lately become her habit to shout quite often at Stepan Trofimovich, but, on the contrary, paid great heed to him, asked him to explain in more detail, herself took Shakespeare and read the immortal chronicle with extreme attention. But the chronicle did not calm her down, nor did  she find all that much resemblance. She waited feverishly for answers to certain of her letters. The answers were not slow in coming; soon the fatal news was  received that Prince Harry had almost simultaneously fought two duels, was entirely to blame for both of them, had killed one of his opponents on the spot and crippled the other, and as a consequence of such deeds had been brought to trial. The affair ended with his being broken to the ranks, stripped of his rights, and exiled to service in one of the infantry regiments, and even that only by special favor.',\n",
       "  'idx': 91,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9765744209289551,\n",
       "  'translator': 'PV'},\n",
       " {'source': '– Вещь короткая; даже, если хотите, по-настоящему это и не анекдот, – посыпался бисер. – Впрочем, романист от безделья мог бы испечь роман. Довольно интересная вещица, Прасковья Ивановна, и я уверен, что Лизавета Николаевна с любопытством выслушает, потому что тут много если не чудных, то причудливых вещей. Лет пять тому, в Петербурге, Николай Всеволодович узнал этого господина, – вот этого самого господина Лебядкина, который стоит разиня рот и, кажется, собирался сейчас улизнуть. Извините, Варвара Петровна. Я вам, впрочем, не советую улепетывать, господин отставной чиновник бывшего провиантского ведомства (видите, я отлично вас помню). И мне и Николаю Всеволодовичу слишком известны ваши здешние проделки, в которых, не забудьте это, вы должны будете дать отчет. Еще раз прошу извинения, Варвара Петровна. Николай Всеволодович называл тогда этого господина своим Фальстафом; это, должно быть (пояснил он вдруг), какой-нибудь бывший характер, burlesque,[105] над которым все смеются и который сам позволяет над собою всем смеяться, лишь бы платили деньги. Николай Всеволодович вел тогда в Петербурге жизнь, так сказать, насмешливую, – другим словом не могу определить ее, потому что в разочарование этот человек не впадет, а делом он и сам тогда пренебрегал заниматься. Я говорю про одно лишь тогдашнее время, Варвара Петровна. У Лебядкина этого была сестра, – вот эта самая, что сейчас здесь сидела. Братец и сестрица не имели своего угла и скитались по чужим. Он бродил под арками Гостиного двора, непременно в бывшем мундире, и останавливал прохожих с виду почище, а что наберет – пропивал. Сестрица же кормилась как птица небесная. Она там в углах помогала и за нужду прислуживала. Содом был ужаснейший; я миную картину этой угловой жизни, – жизни, которой из чудачества предавался тогда и Николай Всеволодович. Я только про тогдашнее время, Варвара Петровна; а что касается до «чудачества», то это его собственное выражение. Он многое от меня не скрывает. Mademoiselle Лебядкина, которой одно время слишком часто пришлось встречать Николая Всеволодовича, была поражена его наружностью. Это был, так сказать, бриллиант на грязном фоне ее жизни. Я плохой описатель чувств, а потому пройду мимо; но ее тотчас же подняли дрянные людишки на смех, и она загрустила. Там вообще над нею смеялись, но прежде она вовсе не замечала того. Голова ее уже и тогда была не в порядке, но тогда все-таки не так, как теперь. Есть основание предположить, что в детстве, через какую-то благодетельницу, она чуть было не получила воспитания. Николай Всеволодович никогда не обращал на нее ни малейшего внимания и играл больше в старые замасленные карты по четверть копейки в преферанс с чиновниками. Но раз, когда ее обижали, он (не спрашивая причины) схватил одного чиновника за шиворот и спустил изо второго этажа в окно. Никаких рыцарских негодований в пользу оскорбленной невинности тут не было; вся операция произошла при общем смехе, и смеялся всех больше Николай Всеволодович сам; когда же всё кончилось благополучно, то помирились и стали пить пунш. Но угнетенная невинность сама про то не забыла. Разумеется, кончилось окончательным сотрясением ее умственных способностей. Повторяю, я плохой описатель чувств, но тут главное мечта. А Николай Всеволодович, как нарочно, еще более раздражал мечту: вместо того чтобы рассмеяться, он вдруг стал обращаться к mademoiselle Лебядкиной с неожиданным уважением. Кириллов, тут бывший (чрезвычайный оригинал, Варвара Петровна, и чрезвычайно отрывистый человек; вы, может быть, когда-нибудь его увидите, он теперь здесь), ну так вот, этот Кириллов, который, по обыкновению, всё молчит, а тут вдруг разгорячился, заметил, я помню, Николаю Всеволодовичу, что тот третирует эту госпожу как маркизу и тем окончательно ее добивает. Прибавлю, что Николай Всеволодович несколько уважал этого Кириллова. Что же, вы думаете, он ему ответил: «Вы полагаете, господин Кириллов, что я смеюсь над нею; разуверьтесь, я в самом деле ее уважаю, потому что она всех нас лучше». И, знаете, таким серьезным тоном сказал. Между тем в эти два-три месяца он, кроме «здравствуйте» да «прощайте», в сущности, не проговорил с ней ни слова. Я, тут бывший, наверно помню, что она до того уже, наконец, дошла, что считала его чем-то вроде жениха своего, не смеющего ее «похитить» единственно потому, что у него много врагов и семейных препятствий или что-то в этом роде. Много тут было смеху! Кончилось тем, что когда Николаю Всеволодовичу пришлось тогда отправляться сюда, он, уезжая, распорядился о ее содержании и, кажется, довольно значительном ежегодном пенсионе, рублей в триста по крайней мере, если не более. Одним словом, положим, всё это с его стороны баловство, фантазия преждевременно уставшего человека, – пусть даже, наконец, как говорил Кириллов, это был новый этюд пресыщенного человека с целью узнать, до чего можно довести сумасшедшую калеку. «Вы, говорит, нарочно выбрали самое последнее существо, калеку, покрытую вечным позором и побоями, – и вдобавок зная, что это существо умирает к вам от комической любви своей, – и вдруг вы нарочно принимаетесь ее морочить, единственно для того, чтобы посмотреть, что из этого выйдет!» Чем, наконец, так особенно виноват человек в фантазиях сумасшедшей женщины, с которой, заметьте, он вряд ли две фразы во всё время выговорил! Есть вещи, Варвара Петровна, о которых не только нельзя умно говорить, но о которых и начинать-то говорить неумно. Ну пусть, наконец, чудачество – но ведь более-то уж ничего нельзя сказать; а между тем теперь вот из этого сделали историю… Мне отчасти известно, Варвара Петровна, о том, что здесь происходит. Рассказчик вдруг оборвал и повернулся было к Лебядкину, но Варвара Петровна остановила его; она была в сильнейшей экзальтации.',\n",
       "  'translation': '\"It\\'s a short matter; in fact, if you like, it\\'s not even an anecdote,\" the beads began spilling out. \"However, a novelist might cook up a novel from it in an idle moment. It\\'s quite an interesting little matter, Praskovya Ivanovna, and I\\'m sure Lizaveta Nikolaevna will listen with curiosity, because there are many things here which, if not queer, are at least quaint. About five years ago, in Petersburg, Nikolai Vsevolodovich got to know this gentleman-this same Mr. Lebyadkin who is standing here with his mouth hanging open and, it seems, was just about to slip away. Forgive me, Varvara Petrovna. Incidentally, I\\'d advise you not to take to your heels, mister retired official of the former supply department (you see, I remember you perfectly). Both I and Nikolai Vsevolodovich are all too well informed of  your local tricks, of which, don\\'t forget, you will have to give an accounting. Once again I ask your forgiveness, Varvara Petrovna. Nikolai Vsevolodovich used to call  this gentleman his Falstaff -that must be some former character,\" he suddenly explained, \"some burlesque everyone laughs at and who allows everyone to laugh at him, so long as they pay money. The life Nikolai Vsevolodovich then led in Petersburg was, so to speak, a jeering one-I cannot define it by any other word, because he was not a man to fall into disillusionment, and he scorned then to do anything serious. I\\'m talking only about that time, Varvara Petrovna. This Lebyadkin had a sister-the very one who was just sitting here. This nice brother and sister had no corner of their own, and wandered about staying with various people. He loitered under the arcades of the Gostiny Dvor, unfailingly wearing his former uniform, and stopped the cleanerlooking passersby, and whatever he collected he would spend on drink. His sister lived like the birds of the air. She helped out in those corners and served in exchange for necessities. It was a most terrible Sodom; I\\'ll pass over the picture of this corner life-the life to which Nikolai Vsevolodovich then gave himself out of whimsicality. This was only then, Varvara Petrovna; and as for \\'whimsicality,\\' the expression is his. There is much that he does not conceal from me. Mademoiselle Lebyadkin, who at a certain period happened to run into Nikolai Vsevolodovich all too often, was struck by his appearance. He was, so to speak, a diamond set against the dirty background of her life. I\\'m a poor describer of feelings, so I\\'ll pass that over; but rotten little people immediately made fun of her, and she grew sad. They generally laughed at her there, but before she didn\\'t notice it. She was already not right in the head then, but less so than now. There\\'s reason to think that in childhood, through some benefactress, she almost received an education. Nikolai Vsevolodovich never paid the slightest attention to her, and rather spent his time playing old greasy cards, the game of preference for quarterkopeck stakes, with some clerks. But once when she was being mistreated, he, without asking why, grabbed one clerk by the scruff of the neck and chucked him out the secondstory window. There wasn\\'t any chivalrous indignation in favor of offended innocence in it; the whole operation took place amid general laughter, and Nikolai Vsevolodovich himself laughed most of all; everything eventually came to a good end, they made peace and began drinking punch. But oppressed innocence herself did not forget it. Of course, it ended with the final shaking of her mental faculties. I repeat, I\\'m a poor describer of feelings, but the main thing here was the dream. And Nikolai Vsevolodovich, as if on purpose, aroused the dream even more; instead of just laughing at it, he suddenly began addressing Mademoiselle Lebyadkin with unexpected esteem. Kirillov, who was there (an exceedingly original man, Varvara Petrovna, and an exceedingly abrupt one; perhaps you\\'ll meet him one day, he\\'s here now), well, so this Kirillov, who ordinarily is always silent, but then suddenly got excited, observed to Nikolai Vsevolodovich, as I remember, that his treating this lady as a marquise was finally going to finish her off. I will add that Nikolai Vsevolodovich had a certain respect for this Kirillov. And how do you think he answered him? \\'You assume, Mr. Kirillov, that I am laughing at her; let me assure you that I do indeed respect her, because she is better than any of us.\\' And, you know, he said it in such a serious tone. Though, in fact, during those two or three months he hadn\\'t said a word to her except \\'hello\\' and \\'goodbye.\\' I, who was there, remember for a certainty that she finally reached the point of regarding him as something like her fiancé, who did not dare to \\'abduct\\' her solely because he had many enemies and family obstacles, or something of the sort. There was much laughter over that! In the end, when Nikolai Vsevolodovich had to come here that time, as he was leaving he arranged for her keep, and it seems it was quite a substantial yearly pension, at least three hundred roubles, if not more. In short, let\\'s say it was all selfindulgence, the fancy of a prematurely weary man-let it be, finally, as Kirillov was saying, a new étude by ajaded man, with the object of finding out what a mad cripple can be brought to. \\'You chose on purpose,\\' he said, \\'the very least of beings, a cripple covered in eternal shame and beatings-and knowing, besides, that this being is dying of her comical love for you-and you suddenly start to flummox her on purpose, solely to see what will come of it!\\' Why, finally, is a man so especially to blame for the fantasy of a mad woman to whom, notice, he had hardly spoken two sentences during that whole time! There are things, Varvara Petrovna, of which it is not only impossible to speak intelligently, but of which it is not intelligent even to begin speaking. Well, let it be whimsicality, finally- but that\\'s all one can say; and yet quite a story has been made of it now... I\\'m partly informed, Varvara Petrovna, of what is going on here.\" The narrator suddenly broke off and was turning to Lebyadkin, but Varvara Petrovna stopped him; she was in the greatest exaltation.',\n",
       "  'idx': 1263,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9765353202819824,\n",
       "  'translator': 'PV'},\n",
       " {'source': '– Да кто? Кто велел вам сюда приходить? – допрашивала Варвара Петровна.',\n",
       "  'translation': '\"But, who? Who told you to come here?\" Varvara Petrovna questioned.',\n",
       "  'idx': 1228,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9762787818908691,\n",
       "  'translator': 'PV'},\n",
       " {'source': 'Так называемое у нас имение Степана Трофимовича (душ пятьдесят по старинному счету, и смежное со Скворешниками) было вовсе не его, а принадлежало первой его супруге, а стало быть, теперь их сыну, Петру Степановичу Верховенскому. Степан Трофимович только опекунствовал, а потому, когда птенец оперился, действовал по формальной от него доверенности на управление имением. Сделка для молодого человека была выгодная: он получал с отца в год до тысячи рублей в виде дохода с имения, тогда как оно при новых порядках не давало и пятисот (а может быть, и того менее). Бог знает как установились подобные отношения. Впрочем, всю эту тысячу целиком высылала Варвара Петровна, а Степан Трофимович ни единым рублем в ней не участвовал. Напротив, весь доход с землицы оставлял у себя в кармане и, кроме того, разорил ее вконец, сдав ее в аренду какому-то промышленнику и, тихонько от Варвары Петровны, продав на сруб рощу, то есть главную ее ценность. Эту рощицу он уже давно продавал урывками. Вся она стоила по крайней мере тысяч восемь, а он взял за нее только пять. Но он иногда слишком много проигрывал в клубе, а просить у Варвары Петровны боялся. Она скрежетала зубами, когда наконец обо всем узнала. И вдруг теперь сынок извещал, что приедет сам продать свои владения во что бы ни стало, а отцу поручал неотлагательно позаботиться о продаже. Ясное дело, что при благородстве и бескорыстии Степана Трофимовича ему стало совестно пред се cher enfant[41] (которого он в последний раз видел целых девять лет тому назад, в Петербурге, студентом). Первоначально все имение могло стоить тысяч тринадцать или четырнадцать, теперь вряд ли кто бы дал за него и пять. Без сомнения, Степан Трофимович имел полное право, по смыслу формальной доверенности, продать лес и, поставив в счет тысячерублевый невозможный ежегодный доход, столько лет высылавшийся аккуратно, сильно оградить себя при расчете. Но Степан Трофимович был благороден, со стремлениями высшими. В голове его мелькнула одна удивительно красивая мысль: когда приедет Петруша, вдруг благородно выложить на стол самый высший maximum цены, то есть даже пятнадцать тысяч, без малейшего намека на высылавшиеся до сих пор суммы, и крепко-крепко, со слезами, прижать к груди се cher fils,[42] чем и покончить все счеты. Отдаленно и осторожно начал он развертывать эту картинку пред Варварой Петровной. Он намекал, что это даже придаст какой-то особый, благородный оттенок их дружеской связи… их «идее». Это выставило бы в таком бескорыстном и великодушном виде прежних отцов и вообще прежних людей сравнительно с новою легкомысленною и социальною молодежью. Много еще он говорил, но Варвара Петровна всё отмалчивалась. Наконец сухо объявила ему, что согласна купить их землю и даст за нее maximum цены, то есть тысяч шесть, семь (и за четыре можно было купить). Об остальных же восьми тысячах, улетевших с рощей, не сказала ни слова.',\n",
       "  'translation': 'Stepan Trofimovich\\'s estate, as we used to call it (about fifty souls by the old way of reckoning, and adjoining Skvoreshniki), was not his at all, but had belonged to his first wife, and so now to their son, Pyotr Stepanovich Verkhovensky. Stepan Trofimovich was merely the trustee, and thus, once the nestling was fully fledged, acted through a formal warrant as manager of the estate. For the young man it was a profitable deal: he received up to a thousand roubles a year from his father as income from the estate, while under the new regulations it did not yield as much as five hundred (and perhaps even less). God knows how such arrangements were set up. However, the entire thousand was sent by Varvara Petrovna, and Stepan Trofimovich did not contribute a single rouble to it. On the contrary, he pocketed all the income from this bit of land, and, furthermore, ruined it altogether by leasing it to some dealer and, in secret from Varvara Petrovna, selling the timber that was its main valuable asset. He had been selling this timber piecemeal for a long time. Its total worth was about eight thousand at least, yet he got only five for it. But he sometimes lost too much at the club, and was afraid to ask Varvara Petrovna. She ground her teeth when she finally learned of it all. And now the boy suddenly notified him that he was coming himself to sell his property at all costs, and charged his father with promptly arranging for the sale. It was clear that Stepan Trofimovich, being a lofty and disinterested man, felt ashamed before ce cher enfant (whom he had last seen as a student in Petersburg all of nine years earlier). Originally, the entire estate might have been worth some thirteen or fourteen thousand, but now it was unlikely that anyone would give five for it. Stepan Trofimovich undoubtedly had every right, in terms of the formal warrant, to sell the timber, and taking into account the impossible annual income of a thousand roubles, which had been sent punctually for so many years, could make a good defense of himself in any final settlement. But Stepan Trofimovich was noble and had lofty aspirations. A remarkably beautiful thought flashed in his head: to lay out nobly on the table, when Petrusha came, the highest maximum of the pricethat is, even fifteen thousand-without the slightest hint at the sums that had been sent previously, and then firmly, very firmly, with tears, to press ce cher fils to his heart, and so settle all accounts. He began remotely and cautiously unfolding this picture before Varvara Petrovna. He hinted that it would even add some special, noble tinge to their friendly connection ... to their \"idea.\" It would show former fathers and former people generally in such a disinterested and magnanimous light, as compared with the new frivolous and social youth. He said many other things, but Varvara Petrovna kept silent. At last she dryly informed him that she would agree to buy their land and would pay the maximum price for it-that is, six or seven thousand (even four would have been enough). Of the remaining eight thousand that had flown away with the timber, she did not say a word.',\n",
       "  'idx': 289,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9760987162590027,\n",
       "  'translator': 'PV'},\n",
       " {'source': '– Шатов? Это брат Дарьи Павловны…',\n",
       "  'translation': '\"Shatov? He is Darya Pavlovna\\'s brother...\"',\n",
       "  'idx': 528,\n",
       "  'book': 'Demons',\n",
       "  'sim': 0.9759403467178345,\n",
       "  'translator': 'PV'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newlist = sorted(translator_to_pars['PV'], key=lambda d: d['sim'], reverse=True) \n",
    "newlist[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the lengths and translator distributions that we've kept and comparing the size of the train and holdout datasets. \n",
    "\n",
    "We had to play around with the books we kept in the holdout set to maintain a reasonable percentage of train/holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All\n",
      "PV 7401\n",
      "Garnett 7401\n",
      "Katz 7401\n",
      "McDuff 7401\n",
      "Total 29604\n",
      "\n",
      "Train\n",
      "PV 6931\n",
      "Garnett 6931\n",
      "Katz 6931\n",
      "McDuff 6931\n",
      "\n",
      "Holdout\n",
      "PV 470\n",
      "Garnett 470\n",
      "Katz 470\n",
      "McDuff 470\n",
      "Train total:  34655\n",
      "Val/Test total:  2350\n",
      "\n",
      "train % =  0.9364950682340224\n",
      "holdout % =  0.06350493176597757\n"
     ]
    }
   ],
   "source": [
    "abs_total = 0\n",
    "print('\\nAll')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    both = len(translator_to_pars_holdout[k]) + len(translator_to_pars[k])\n",
    "    print(k, both)\n",
    "    abs_total += both\n",
    "print('Total', abs_total)\n",
    "\n",
    "train_total = 0\n",
    "min_class = 100000000000\n",
    "print('\\nTrain')\n",
    "for k in translator_to_pars.keys():\n",
    "    print(k, len(translator_to_pars[k]))\n",
    "    if len(translator_to_pars[k]) < min_class:\n",
    "        min_class = len(translator_to_pars[k])\n",
    "    \n",
    "train_total = len(translator_to_pars.keys()) * min_class\n",
    "\n",
    "holdout_total = 0\n",
    "min_class_h = 100000000000\n",
    "print('\\nHoldout')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    print(k, len(translator_to_pars_holdout[k]))\n",
    "    if len(translator_to_pars_holdout[k]) < min_class_h:\n",
    "        min_class_h = len(translator_to_pars_holdout[k])\n",
    "\n",
    "holdout_total = len(translator_to_pars.keys()) * min_class_h\n",
    "\n",
    "print('Train total: ', min_class*5)\n",
    "print('Val/Test total: ', min_class_h*5)\n",
    "print()\n",
    "print('train % = ', train_total/(holdout_total+train_total))\n",
    "print('holdout % = ', holdout_total/(holdout_total+train_total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping Data for Pre-processing for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(list(translator_to_pars.keys()))\n",
    "print(le.transform([\"Garnett\", \"McDuff\", \"PV\", \"Katz\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "i = 0\n",
    "for tr in translator_to_pars.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat,  'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list.append(sent_dict)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "data_list_holdout = []\n",
    "i = 0\n",
    "for tr in translator_to_pars_holdout.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars_holdout[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat, 'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list_holdout.append(sent_dict)\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>book</th>\n",
       "      <th>labels</th>\n",
       "      <th>concat</th>\n",
       "      <th>translator</th>\n",
       "      <th>sim</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>Demons</td>\n",
       "      <td>3</td>\n",
       "      <td>Из лицея молодой человек в первые два года при...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976574</td>\n",
       "      <td>Из лицея молодой человек в первые два года при...</td>\n",
       "      <td>For the first two years the young man came hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1263</td>\n",
       "      <td>Demons</td>\n",
       "      <td>3</td>\n",
       "      <td>– Вещь короткая; даже, если хотите, по-настоящ...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976535</td>\n",
       "      <td>– Вещь короткая; даже, если хотите, по-настоящ...</td>\n",
       "      <td>\"It's a short matter; in fact, if you like, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1228</td>\n",
       "      <td>Demons</td>\n",
       "      <td>3</td>\n",
       "      <td>– Да кто? Кто велел вам сюда приходить? – допр...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976279</td>\n",
       "      <td>– Да кто? Кто велел вам сюда приходить? – допр...</td>\n",
       "      <td>\"But, who? Who told you to come here?\" Varvara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>289</td>\n",
       "      <td>Demons</td>\n",
       "      <td>3</td>\n",
       "      <td>Так называемое у нас имение Степана Трофимович...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.976099</td>\n",
       "      <td>Так называемое у нас имение Степана Трофимович...</td>\n",
       "      <td>Stepan Trofimovich's estate, as we used to cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>528</td>\n",
       "      <td>Demons</td>\n",
       "      <td>3</td>\n",
       "      <td>– Шатов? Это брат Дарьи Павловны… &lt;SEP&gt; \"Shato...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.975940</td>\n",
       "      <td>– Шатов? Это брат Дарьи Павловны…</td>\n",
       "      <td>\"Shatov? He is Darya Pavlovna's brother...\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx    book  labels                                             concat  \\\n",
       "0    91  Demons       3  Из лицея молодой человек в первые два года при...   \n",
       "1  1263  Demons       3  – Вещь короткая; даже, если хотите, по-настоящ...   \n",
       "2  1228  Demons       3  – Да кто? Кто велел вам сюда приходить? – допр...   \n",
       "3   289  Demons       3  Так называемое у нас имение Степана Трофимович...   \n",
       "4   528  Demons       3  – Шатов? Это брат Дарьи Павловны… <SEP> \"Shato...   \n",
       "\n",
       "  translator       sim                                                src  \\\n",
       "0         PV  0.976574  Из лицея молодой человек в первые два года при...   \n",
       "1         PV  0.976535  – Вещь короткая; даже, если хотите, по-настоящ...   \n",
       "2         PV  0.976279  – Да кто? Кто велел вам сюда приходить? – допр...   \n",
       "3         PV  0.976099  Так называемое у нас имение Степана Трофимович...   \n",
       "4         PV  0.975940                  – Шатов? Это брат Дарьи Павловны…   \n",
       "\n",
       "                                                 tgt  \n",
       "0  For the first two years the young man came hom...  \n",
       "1  \"It's a short matter; in fact, if you like, it...  \n",
       "2  \"But, who? Who told you to come here?\" Varvara...  \n",
       "3  Stepan Trofimovich's estate, as we used to cal...  \n",
       "4        \"Shatov? He is Darya Pavlovna's brother...\"  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df_holdout = pd.DataFrame(data_list_holdout)\n",
    "df_holdout_X = df_holdout[['idx','book', 'concat', 'translator', 'sim', 'src', 'tgt']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27724, 8)\n",
      "(1880, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  (27724, 8)\n",
      "val size:  (940, 8)\n",
      "test size:  (940, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_texts, val_texts, test_labels, val_labels = train_test_split(\n",
    "    df_holdout_X, df_holdout['labels'],\n",
    "    stratify = df_holdout['labels'], shuffle=True, test_size=0.5\n",
    ")\n",
    "\n",
    "aligned_train_df = df\n",
    "test_df = pd.concat([test_texts, test_labels], axis=1)\n",
    "val_df = pd.concat([val_texts, val_labels], axis=1)\n",
    "print('train size: ', aligned_train_df.shape)\n",
    "print('val size: ', val_df.shape)\n",
    "print('test size: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE ALIGNED TRAIN\n",
    "aligned_train_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/4class_same_holdout_aligned_train_df.pickle\")  \n",
    "\n",
    "# SAVE HOLDOUT VAL\n",
    "val_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/4class_same_holdout_experiment_val_df.pickle\")  \n",
    "\n",
    "# SAVE HOLDOUT TEST\n",
    "test_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/4class_same_holdout_experiment_test_df.pickle\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>book</th>\n",
       "      <th>concat</th>\n",
       "      <th>translator</th>\n",
       "      <th>sim</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>4484</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Так я и порешил, чтоб ни за что, парень, и н...</td>\n",
       "      <td>McDuff</td>\n",
       "      <td>0.870930</td>\n",
       "      <td>– Так я и порешил, чтоб ни за что, парень, и н...</td>\n",
       "      <td>'That's what I've decided, not on any account,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>59</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– В Сибирь, в Сибирь! Тотчас в Сибирь! &lt;SEP&gt; \"...</td>\n",
       "      <td>McDuff</td>\n",
       "      <td>0.949025</td>\n",
       "      <td>– В Сибирь, в Сибирь! Тотчас в Сибирь!</td>\n",
       "      <td>\"Yes indeed, to Siberia, Siberia! Straight to ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>2155</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Вчера утром, – отрапортовал Келлер, – мы име...</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.892784</td>\n",
       "      <td>– Вчера утром, – отрапортовал Келлер, – мы име...</td>\n",
       "      <td>\"Yesterday morning,\" answered Keller, \"we met ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>2349</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>Князь молчал. &lt;SEP&gt; Myshkin did not speak.</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.514383</td>\n",
       "      <td>Князь молчал.</td>\n",
       "      <td>Myshkin did not speak.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>3546</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– И наверно дошла, – заметил Ганя. &lt;SEP&gt; 'It p...</td>\n",
       "      <td>McDuff</td>\n",
       "      <td>0.723232</td>\n",
       "      <td>– И наверно дошла, – заметил Ганя.</td>\n",
       "      <td>'It probably has reached her ears,' observed G...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>3683</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Философия нужна-с, очень бы нужна была-с в н...</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.834038</td>\n",
       "      <td>– Философия нужна-с, очень бы нужна была-с в н...</td>\n",
       "      <td>\"Philosophy would be useful, very useful in ou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>501</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Но только так, чтобы никто не заметил, – умо...</td>\n",
       "      <td>McDuff</td>\n",
       "      <td>0.829551</td>\n",
       "      <td>– Но только так, чтобы никто не заметил, – умо...</td>\n",
       "      <td>'Only it must be done so no one notices,' Gany...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>325</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Игумен Пафнутий, – отвечал князь внимательно...</td>\n",
       "      <td>McDuff</td>\n",
       "      <td>0.740250</td>\n",
       "      <td>– Игумен Пафнутий, – отвечал князь внимательно...</td>\n",
       "      <td>'The Abbot Pafnuty,' the prince replied, with ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1655</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Ты. Она тебя тогда, с тех самых пор, с имени...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>– Ты. Она тебя тогда, с тех самых пор, с имени...</td>\n",
       "      <td>\"You. She fell in love with you then, ever sin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>3625</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Но вы бог знает что из самого обыкновенного ...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.857332</td>\n",
       "      <td>– Но вы бог знает что из самого обыкновенного ...</td>\n",
       "      <td>\"But you make God knows what out of a most ord...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>319</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Пафнутий? Игумен? Да постойте, постойте, куд...</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.726148</td>\n",
       "      <td>– Пафнутий? Игумен? Да постойте, постойте, куд...</td>\n",
       "      <td>\"Pafnuty? The abbot? Stop a minute—stop a minu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>1848</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Да разве я один? – не умолкал Коля. – Все то...</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.941908</td>\n",
       "      <td>– Да разве я один? – не умолкал Коля. – Все то...</td>\n",
       "      <td>\"But am I the only one?\" Kolya persisted. \"Eve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>229</td>\n",
       "      <td>NotesFromUnderground</td>\n",
       "      <td>Но я не пел. Я старался только ни на кого из н...</td>\n",
       "      <td>Katz</td>\n",
       "      <td>0.907434</td>\n",
       "      <td>Но я не пел. Я старался только ни на кого из н...</td>\n",
       "      <td>hmm.\" But I didn't sing. I just tried not to l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>139</td>\n",
       "      <td>NotesFromUnderground</td>\n",
       "      <td>—Я вас не задерживаю ли?— спросил я после двух...</td>\n",
       "      <td>Katz</td>\n",
       "      <td>0.485020</td>\n",
       "      <td>—Я вас не задерживаю ли?— спросил я после двух...</td>\n",
       "      <td>\"?'m not detaining you, am I?\" I asked after a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>467</td>\n",
       "      <td>NotesFromUnderground</td>\n",
       "      <td>—Будет!— кричал я,— даю тебе честное слово мое...</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.597289</td>\n",
       "      <td>—Будет!— кричал я,— даю тебе честное слово мое...</td>\n",
       "      <td>\"It shall be so,\" I said, \"I give you my word ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1044</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Войдемте, это так, – бормотал генерал князю,...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.767762</td>\n",
       "      <td>– Войдемте, это так, – бормотал генерал князю,...</td>\n",
       "      <td>\"Let's go in, it's all right,\" the general mur...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>3744</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Ваша книга, которую я брал у вас намедни, – ...</td>\n",
       "      <td>McDuff</td>\n",
       "      <td>0.883834</td>\n",
       "      <td>– Ваша книга, которую я брал у вас намедни, – ...</td>\n",
       "      <td>'Your book, the one I borrowed from you yester...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>109</td>\n",
       "      <td>NotesFromUnderground</td>\n",
       "      <td>Был, впрочем, у меня и еще как будто один знак...</td>\n",
       "      <td>Katz</td>\n",
       "      <td>0.922268</td>\n",
       "      <td>Был, впрочем, у меня и еще как будто один знак...</td>\n",
       "      <td>Thad one other sort of acquaintance, however, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>3751</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>– Вы имеете свою квартиру, в Павловске, у… У д...</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.832582</td>\n",
       "      <td>– Вы имеете свою квартиру, в Павловске, у… У д...</td>\n",
       "      <td>\"You have your own rooms at Pavlovsk at . ..at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1665</td>\n",
       "      <td>TheIdiot</td>\n",
       "      <td>Говоря, князь в рассеянности опять было захват...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.855467</td>\n",
       "      <td>Говоря, князь в рассеянности опять было захват...</td>\n",
       "      <td>As he was talking, the prince had again absent...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                  book  \\\n",
       "1855  4484              TheIdiot   \n",
       "1760    59              TheIdiot   \n",
       "882   2155              TheIdiot   \n",
       "718   2349              TheIdiot   \n",
       "1675  3546              TheIdiot   \n",
       "711   3683              TheIdiot   \n",
       "1645   501              TheIdiot   \n",
       "1547   325              TheIdiot   \n",
       "253   1655              TheIdiot   \n",
       "81    3625              TheIdiot   \n",
       "858    319              TheIdiot   \n",
       "700   1848              TheIdiot   \n",
       "1172   229  NotesFromUnderground   \n",
       "1000   139  NotesFromUnderground   \n",
       "674    467  NotesFromUnderground   \n",
       "106   1044              TheIdiot   \n",
       "1864  3744              TheIdiot   \n",
       "1067   109  NotesFromUnderground   \n",
       "592   3751              TheIdiot   \n",
       "272   1665              TheIdiot   \n",
       "\n",
       "                                                 concat translator       sim  \\\n",
       "1855  – Так я и порешил, чтоб ни за что, парень, и н...     McDuff  0.870930   \n",
       "1760  – В Сибирь, в Сибирь! Тотчас в Сибирь! <SEP> \"...     McDuff  0.949025   \n",
       "882   – Вчера утром, – отрапортовал Келлер, – мы име...    Garnett  0.892784   \n",
       "718          Князь молчал. <SEP> Myshkin did not speak.    Garnett  0.514383   \n",
       "1675  – И наверно дошла, – заметил Ганя. <SEP> 'It p...     McDuff  0.723232   \n",
       "711   – Философия нужна-с, очень бы нужна была-с в н...    Garnett  0.834038   \n",
       "1645  – Но только так, чтобы никто не заметил, – умо...     McDuff  0.829551   \n",
       "1547  – Игумен Пафнутий, – отвечал князь внимательно...     McDuff  0.740250   \n",
       "253   – Ты. Она тебя тогда, с тех самых пор, с имени...         PV  0.927536   \n",
       "81    – Но вы бог знает что из самого обыкновенного ...         PV  0.857332   \n",
       "858   – Пафнутий? Игумен? Да постойте, постойте, куд...    Garnett  0.726148   \n",
       "700   – Да разве я один? – не умолкал Коля. – Все то...    Garnett  0.941908   \n",
       "1172  Но я не пел. Я старался только ни на кого из н...       Katz  0.907434   \n",
       "1000  —Я вас не задерживаю ли?— спросил я после двух...       Katz  0.485020   \n",
       "674   —Будет!— кричал я,— даю тебе честное слово мое...    Garnett  0.597289   \n",
       "106   – Войдемте, это так, – бормотал генерал князю,...         PV  0.767762   \n",
       "1864  – Ваша книга, которую я брал у вас намедни, – ...     McDuff  0.883834   \n",
       "1067  Был, впрочем, у меня и еще как будто один знак...       Katz  0.922268   \n",
       "592   – Вы имеете свою квартиру, в Павловске, у… У д...    Garnett  0.832582   \n",
       "272   Говоря, князь в рассеянности опять было захват...         PV  0.855467   \n",
       "\n",
       "                                                    src  \\\n",
       "1855  – Так я и порешил, чтоб ни за что, парень, и н...   \n",
       "1760             – В Сибирь, в Сибирь! Тотчас в Сибирь!   \n",
       "882   – Вчера утром, – отрапортовал Келлер, – мы име...   \n",
       "718                                       Князь молчал.   \n",
       "1675                 – И наверно дошла, – заметил Ганя.   \n",
       "711   – Философия нужна-с, очень бы нужна была-с в н...   \n",
       "1645  – Но только так, чтобы никто не заметил, – умо...   \n",
       "1547  – Игумен Пафнутий, – отвечал князь внимательно...   \n",
       "253   – Ты. Она тебя тогда, с тех самых пор, с имени...   \n",
       "81    – Но вы бог знает что из самого обыкновенного ...   \n",
       "858   – Пафнутий? Игумен? Да постойте, постойте, куд...   \n",
       "700   – Да разве я один? – не умолкал Коля. – Все то...   \n",
       "1172  Но я не пел. Я старался только ни на кого из н...   \n",
       "1000  —Я вас не задерживаю ли?— спросил я после двух...   \n",
       "674   —Будет!— кричал я,— даю тебе честное слово мое...   \n",
       "106   – Войдемте, это так, – бормотал генерал князю,...   \n",
       "1864  – Ваша книга, которую я брал у вас намедни, – ...   \n",
       "1067  Был, впрочем, у меня и еще как будто один знак...   \n",
       "592   – Вы имеете свою квартиру, в Павловске, у… У д...   \n",
       "272   Говоря, князь в рассеянности опять было захват...   \n",
       "\n",
       "                                                    tgt  labels  \n",
       "1855  'That's what I've decided, not on any account,...       2  \n",
       "1760  \"Yes indeed, to Siberia, Siberia! Straight to ...       2  \n",
       "882   \"Yesterday morning,\" answered Keller, \"we met ...       0  \n",
       "718                              Myshkin did not speak.       0  \n",
       "1675  'It probably has reached her ears,' observed G...       2  \n",
       "711   \"Philosophy would be useful, very useful in ou...       0  \n",
       "1645  'Only it must be done so no one notices,' Gany...       2  \n",
       "1547  'The Abbot Pafnuty,' the prince replied, with ...       2  \n",
       "253   \"You. She fell in love with you then, ever sin...       3  \n",
       "81    \"But you make God knows what out of a most ord...       3  \n",
       "858   \"Pafnuty? The abbot? Stop a minute—stop a minu...       0  \n",
       "700   \"But am I the only one?\" Kolya persisted. \"Eve...       0  \n",
       "1172  hmm.\" But I didn't sing. I just tried not to l...       1  \n",
       "1000  \"?'m not detaining you, am I?\" I asked after a...       1  \n",
       "674   \"It shall be so,\" I said, \"I give you my word ...       0  \n",
       "106   \"Let's go in, it's all right,\" the general mur...       3  \n",
       "1864  'Your book, the one I borrowed from you yester...       2  \n",
       "1067  Thad one other sort of acquaintance, however, ...       1  \n",
       "592   \"You have your own rooms at Pavlovsk at . ..at...       0  \n",
       "272   As he was talking, the prince had again absent...       3  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Experiment Dataset without Sorting/Filtering\n",
    "\n",
    "To compare the efficacy of sorting/filtering alignments, we want to compare performance with randomly sampled data with the same dataset size and holdout parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_paragraph_len = 20\n",
    "max_paragraph_len = 1000000000000\n",
    "top_k_percent = 1\n",
    "num_k = 5000\n",
    "drop_top = 0.00\n",
    "align_scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_books = ['TheIdiot', 'NotesFromUnderground']\n",
    "ignore_books = []\n",
    "ignore_translator = ['Hogarth']\n",
    "translator_to_pars = {}\n",
    "translator_to_pars_holdout = {}\n",
    "\n",
    "# for each book in train:\n",
    "for book in sorted(list(aligned_paragraph_dataset.keys())):\n",
    "    # get par list of aligned sentences, best k alignments\n",
    "    book_par_list = [list(aligned_paragraph_dataset[book][p].values()) for p in range(len(aligned_paragraph_dataset[book]))]\n",
    "    source_par_list = source_paragraph_dataset[book]\n",
    "\n",
    "    if book in holdout_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, 1.0, 5000, 0, 'sim', min_paragraph_len, max_paragraph_len, align_scale)\n",
    "    elif book not in ignore_books:\n",
    "        i2score, top_k, keep_idx = get_best_alignments(book_par_list, source_par_list, top_k_percent, num_k, drop_top, 'sim', min_paragraph_len, max_paragraph_len, align_scale)\n",
    "    else:\n",
    "        top_k = []\n",
    "\n",
    "    for i, sim in top_k:\n",
    "        par_trans_dict = aligned_paragraph_dataset[book][i]\n",
    "        par_source = source_paragraph_dataset[book][i]\n",
    "\n",
    "        for translator, t in par_trans_dict.items():\n",
    "            if translator not in ignore_translator:\n",
    "                t = t.replace('\\\\\\'', '\\'')\n",
    "                datum_dict = {'source':par_source, 'translation': t, 'idx': i, 'book': book, 'sim': sim, 'translator': translator}\n",
    "\n",
    "                if translator not in translator_to_pars.keys():\n",
    "                    translator_to_pars[translator] = []\n",
    "                    translator_to_pars_holdout[translator] = []\n",
    "                    \n",
    "                if book in holdout_books:\n",
    "                    translator_to_pars_holdout[translator].append(datum_dict)\n",
    "                    # print('len par_list: ', len(book_par_list))\n",
    "                    # print('len top_k: ', len(top_k))\n",
    "                else:\n",
    "                    translator_to_pars[translator].append(datum_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entire_dataset = 0\n",
    "holdout_entire_dataset = 0\n",
    "for t in translator_to_pars.keys():\n",
    "    train_entire_dataset += len(translator_to_pars[t])\n",
    "    holdout_entire_dataset += len(translator_to_pars_holdout[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27724"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6931\n",
      "470\n"
     ]
    }
   ],
   "source": [
    "min_len = int(aligned_train_df.shape[0]/4)\n",
    "print(min_len)\n",
    "for t in translator_to_pars.keys():\n",
    "    keep = sample(translator_to_pars[t], min_len) \n",
    "    translator_to_pars[t] = keep\n",
    "\n",
    "min_len_h = len(translator_to_pars_holdout['Katz'])\n",
    "print(min_len_h)\n",
    "for t in translator_to_pars_holdout.keys():\n",
    "    keep = sample(translator_to_pars_holdout[t], min_len_h) \n",
    "    translator_to_pars_holdout[t] = keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train + Holdout\n",
      "PV 7401\n",
      "Garnett 7401\n",
      "Katz 7401\n",
      "McDuff 7401\n",
      "Total 29604\n",
      "\n",
      "Train\n",
      "PV 6931\n",
      "Garnett 6931\n",
      "Katz 6931\n",
      "McDuff 6931\n",
      "\n",
      "Holdout\n",
      "PV 470\n",
      "Garnett 470\n",
      "Katz 470\n",
      "McDuff 470\n",
      "Train total:  34655\n",
      "Val/Test total:  2350\n",
      "\n",
      "train % =  0.9364950682340224\n",
      "holdout % =  0.06350493176597757\n",
      "\n",
      "entire dataset % =  0.41511021369678613\n",
      "entire train % =  0.4854661343419486\n",
      "holdout train % =  0.13231981981981983\n"
     ]
    }
   ],
   "source": [
    "abs_total = 0\n",
    "print('\\nTrain + Holdout')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    both = len(translator_to_pars_holdout[k]) + len(translator_to_pars[k])\n",
    "    print(k, both)\n",
    "    abs_total += both\n",
    "print('Total', abs_total)\n",
    "\n",
    "train_total = 0\n",
    "min_class = 100000000000\n",
    "print('\\nTrain')\n",
    "for k in translator_to_pars.keys():\n",
    "    print(k, len(translator_to_pars[k]))\n",
    "    if len(translator_to_pars[k]) < min_class:\n",
    "        min_class = len(translator_to_pars[k])\n",
    "    \n",
    "train_total = len(translator_to_pars.keys()) * min_class\n",
    "\n",
    "holdout_total = 0\n",
    "min_class_h = 100000000000\n",
    "print('\\nHoldout')\n",
    "for k in translator_to_pars_holdout.keys():\n",
    "    print(k, len(translator_to_pars_holdout[k]))\n",
    "    if len(translator_to_pars_holdout[k]) < min_class_h:\n",
    "        min_class_h = len(translator_to_pars_holdout[k])\n",
    "\n",
    "holdout_total = len(translator_to_pars.keys()) * min_class_h\n",
    "\n",
    "print('Train total: ', min_class*5)\n",
    "print('Val/Test total: ', min_class_h*5)\n",
    "print()\n",
    "print('train % = ', train_total/(holdout_total+train_total))\n",
    "print('holdout % = ', holdout_total/(holdout_total+train_total))\n",
    "print()\n",
    "print('entire dataset % = ', (holdout_total+train_total)/(train_entire_dataset + holdout_entire_dataset))\n",
    "print('entire train % = ', (train_total)/(train_entire_dataset))\n",
    "print('holdout train % = ', (holdout_total)/(holdout_entire_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "i = 0\n",
    "for tr in translator_to_pars.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for d in translator_to_pars[tr]:\n",
    "        src, tgt = d['source'], d['translation']\n",
    "        concat = src + ' <SEP> ' + tgt\n",
    "        sent_dict = {'idx': d['idx'], 'book':d['book'], 'labels': label, 'concat': concat,  'translator': d['translator'], 'sim': d['sim'], 'src': src, 'tgt': tgt}\n",
    "        data_list.append(sent_dict)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27724, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data_list)\n",
    "random_train_df = df\n",
    "random_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE ALIGNED TRAIN\n",
    "random_train_df.to_pickle(\"/home/kkatsy/litMT/experiment_dataset/4class_same_holdout_random_train_df.pickle\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>book</th>\n",
       "      <th>labels</th>\n",
       "      <th>concat</th>\n",
       "      <th>translator</th>\n",
       "      <th>sim</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20239</th>\n",
       "      <td>2980</td>\n",
       "      <td>CrimeAndPunishment</td>\n",
       "      <td>1</td>\n",
       "      <td>– Нет, нет; никогда и нигде! – вскрикнула Соня...</td>\n",
       "      <td>Katz</td>\n",
       "      <td>0.860224</td>\n",
       "      <td>– Нет, нет; никогда и нигде! – вскрикнула Соня...</td>\n",
       "      <td>\"No, no; never and nowhere!\" Sonya cried. \"I'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>3619</td>\n",
       "      <td>AnnaKarenina</td>\n",
       "      <td>3</td>\n",
       "      <td>Степан Аркадьич широко открыл свои блестящие, ...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.918005</td>\n",
       "      <td>Степан Аркадьич широко открыл свои блестящие, ...</td>\n",
       "      <td>Stepan Arkadyich opened his shining, clear eye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16479</th>\n",
       "      <td>4422</td>\n",
       "      <td>TheBrothersKaramazov</td>\n",
       "      <td>1</td>\n",
       "      <td>– Про кого ты говоришь… про Митю? – в недоумен...</td>\n",
       "      <td>Katz</td>\n",
       "      <td>0.847434</td>\n",
       "      <td>– Про кого ты говоришь… про Митю? – в недоумен...</td>\n",
       "      <td>\"Who are you talking about . .. about Mitya?\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3512</th>\n",
       "      <td>762</td>\n",
       "      <td>AnnaKarenina</td>\n",
       "      <td>3</td>\n",
       "      <td>– Что делать, придумай, Анна, помоги. Я все пе...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.853283</td>\n",
       "      <td>– Что делать, придумай, Анна, помоги. Я все пе...</td>\n",
       "      <td>'What's to be done, think, Anna, help me. I've...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9608</th>\n",
       "      <td>1135</td>\n",
       "      <td>CrimeAndPunishment</td>\n",
       "      <td>0</td>\n",
       "      <td>– Фатеру по ночам не нанимают; а к тому же вы ...</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.463574</td>\n",
       "      <td>– Фатеру по ночам не нанимают; а к тому же вы ...</td>\n",
       "      <td>and you ought to come up with the porter.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>3557</td>\n",
       "      <td>Demons</td>\n",
       "      <td>3</td>\n",
       "      <td>– Вам-то что за дело? – спросил он вдруг с стр...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.985460</td>\n",
       "      <td>– Вам-то что за дело? – спросил он вдруг с стр...</td>\n",
       "      <td>\"What business is it of yours?\" he asked sudde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27553</th>\n",
       "      <td>5324</td>\n",
       "      <td>TheBrothersKaramazov</td>\n",
       "      <td>2</td>\n",
       "      <td>– А простить не хочешь! – прокричал Митя Груше...</td>\n",
       "      <td>McDuff</td>\n",
       "      <td>0.778375</td>\n",
       "      <td>– А простить не хочешь! – прокричал Митя Груше...</td>\n",
       "      <td>'But you do not want to forgive!' Mitya shoute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7942</th>\n",
       "      <td>2588</td>\n",
       "      <td>CrimeAndPunishment</td>\n",
       "      <td>0</td>\n",
       "      <td>– Да не беспокойтесь же о форме, – перебил Пор...</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.800496</td>\n",
       "      <td>– Да не беспокойтесь же о форме, – перебил Пор...</td>\n",
       "      <td>\"Don't worry about the form,\" Porfiry interrup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5266</th>\n",
       "      <td>1931</td>\n",
       "      <td>CrimeAndPunishment</td>\n",
       "      <td>3</td>\n",
       "      <td>– Ну, полноте, кто ж у нас на Руси себя Наполе...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.745844</td>\n",
       "      <td>– Ну, полноте, кто ж у нас на Руси себя Наполе...</td>\n",
       "      <td>\"But, my goodness, who in our Russia nowadays ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10236</th>\n",
       "      <td>3188</td>\n",
       "      <td>CrimeAndPunishment</td>\n",
       "      <td>0</td>\n",
       "      <td>– Да что вы, Родион Романыч, такой сам не свой...</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.823364</td>\n",
       "      <td>– Да что вы, Родион Романыч, такой сам не свой...</td>\n",
       "      <td>Svidrigailov looked intently at Raskolnikov an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24260</th>\n",
       "      <td>3195</td>\n",
       "      <td>CrimeAndPunishment</td>\n",
       "      <td>2</td>\n",
       "      <td>– Что же ты теперь хочешь делать? &lt;SEP&gt; 'What ...</td>\n",
       "      <td>McDuff</td>\n",
       "      <td>0.733928</td>\n",
       "      <td>– Что же ты теперь хочешь делать?</td>\n",
       "      <td>'What are you going to do?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7073</th>\n",
       "      <td>350</td>\n",
       "      <td>PoorFolk</td>\n",
       "      <td>0</td>\n",
       "      <td>Милостивая государыня, Варвара Алексеевна! &lt;SE...</td>\n",
       "      <td>Garnett</td>\n",
       "      <td>0.504194</td>\n",
       "      <td>Милостивая государыня, Варвара Алексеевна!</td>\n",
       "      <td>DEAR VARVARA ALEXYEVNA,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6655</th>\n",
       "      <td>737</td>\n",
       "      <td>AnnaKarenina</td>\n",
       "      <td>3</td>\n",
       "      <td>Осмотрев детей, они сели, уже одни, в гостиной...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.902843</td>\n",
       "      <td>Осмотрев детей, они сели, уже одни, в гостиной...</td>\n",
       "      <td>After looking at the children, they sat down, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659</th>\n",
       "      <td>3610</td>\n",
       "      <td>CrimeAndPunishment</td>\n",
       "      <td>3</td>\n",
       "      <td>– А-зе здеся нельзя, здеся не места! – встрепе...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.669192</td>\n",
       "      <td>– А-зе здеся нельзя, здеся не места! – встрепе...</td>\n",
       "      <td>Achilles roused himself, his pupils widening m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>1103</td>\n",
       "      <td>TheBrothersKaramazov</td>\n",
       "      <td>3</td>\n",
       "      <td>– Я… я спрошу его… – пробормотал Алеша. – Если...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.827257</td>\n",
       "      <td>– Я… я спрошу его… – пробормотал Алеша. – Если...</td>\n",
       "      <td>I... I'll ask him,\" Alyosha murmured. \"If it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>5589</td>\n",
       "      <td>AnnaKarenina</td>\n",
       "      <td>3</td>\n",
       "      <td>– Зовут водку пить. Они, верно, луга делили. Я...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.819945</td>\n",
       "      <td>– Зовут водку пить. Они, верно, луга делили. Я...</td>\n",
       "      <td>'They're inviting us to drink vodka. They've p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>3129</td>\n",
       "      <td>TheBrothersKaramazov</td>\n",
       "      <td>3</td>\n",
       "      <td>– Да помилуйте же, господа! Ну, взял пестик… Н...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.833740</td>\n",
       "      <td>– Да помилуйте же, господа! Ну, взял пестик… Н...</td>\n",
       "      <td>\"But for pity's sake, gentlemen! So I took the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24597</th>\n",
       "      <td>3617</td>\n",
       "      <td>TheBrothersKaramazov</td>\n",
       "      <td>2</td>\n",
       "      <td>– Запоздал, – ответил Красоткин. – Есть обстоя...</td>\n",
       "      <td>McDuff</td>\n",
       "      <td>0.818035</td>\n",
       "      <td>– Запоздал, – ответил Красоткин. – Есть обстоя...</td>\n",
       "      <td>'I was delayed,' Krasotkin replied. 'There wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>2848</td>\n",
       "      <td>Demons</td>\n",
       "      <td>3</td>\n",
       "      <td>– А, черт, да вы его в христианскую веру обрат...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.790643</td>\n",
       "      <td>– А, черт, да вы его в христианскую веру обрат...</td>\n",
       "      <td>\"Ah, the devil, you'll convert him to the Chri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>4113</td>\n",
       "      <td>Demons</td>\n",
       "      <td>3</td>\n",
       "      <td>– По моему мнению… подобная прокламация… одна ...</td>\n",
       "      <td>PV</td>\n",
       "      <td>0.891061</td>\n",
       "      <td>– По моему мнению… подобная прокламация… одна ...</td>\n",
       "      <td>\"In my opinion... such a tract ... is nothing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx                  book  labels  \\\n",
       "20239  2980    CrimeAndPunishment       1   \n",
       "1157   3619          AnnaKarenina       3   \n",
       "16479  4422  TheBrothersKaramazov       1   \n",
       "3512    762          AnnaKarenina       3   \n",
       "9608   1135    CrimeAndPunishment       0   \n",
       "3153   3557                Demons       3   \n",
       "27553  5324  TheBrothersKaramazov       2   \n",
       "7942   2588    CrimeAndPunishment       0   \n",
       "5266   1931    CrimeAndPunishment       3   \n",
       "10236  3188    CrimeAndPunishment       0   \n",
       "24260  3195    CrimeAndPunishment       2   \n",
       "7073    350              PoorFolk       0   \n",
       "6655    737          AnnaKarenina       3   \n",
       "4659   3610    CrimeAndPunishment       3   \n",
       "757    1103  TheBrothersKaramazov       3   \n",
       "5118   5589          AnnaKarenina       3   \n",
       "3187   3129  TheBrothersKaramazov       3   \n",
       "24597  3617  TheBrothersKaramazov       2   \n",
       "1354   2848                Demons       3   \n",
       "2158   4113                Demons       3   \n",
       "\n",
       "                                                  concat translator       sim  \\\n",
       "20239  – Нет, нет; никогда и нигде! – вскрикнула Соня...       Katz  0.860224   \n",
       "1157   Степан Аркадьич широко открыл свои блестящие, ...         PV  0.918005   \n",
       "16479  – Про кого ты говоришь… про Митю? – в недоумен...       Katz  0.847434   \n",
       "3512   – Что делать, придумай, Анна, помоги. Я все пе...         PV  0.853283   \n",
       "9608   – Фатеру по ночам не нанимают; а к тому же вы ...    Garnett  0.463574   \n",
       "3153   – Вам-то что за дело? – спросил он вдруг с стр...         PV  0.985460   \n",
       "27553  – А простить не хочешь! – прокричал Митя Груше...     McDuff  0.778375   \n",
       "7942   – Да не беспокойтесь же о форме, – перебил Пор...    Garnett  0.800496   \n",
       "5266   – Ну, полноте, кто ж у нас на Руси себя Наполе...         PV  0.745844   \n",
       "10236  – Да что вы, Родион Романыч, такой сам не свой...    Garnett  0.823364   \n",
       "24260  – Что же ты теперь хочешь делать? <SEP> 'What ...     McDuff  0.733928   \n",
       "7073   Милостивая государыня, Варвара Алексеевна! <SE...    Garnett  0.504194   \n",
       "6655   Осмотрев детей, они сели, уже одни, в гостиной...         PV  0.902843   \n",
       "4659   – А-зе здеся нельзя, здеся не места! – встрепе...         PV  0.669192   \n",
       "757    – Я… я спрошу его… – пробормотал Алеша. – Если...         PV  0.827257   \n",
       "5118   – Зовут водку пить. Они, верно, луга делили. Я...         PV  0.819945   \n",
       "3187   – Да помилуйте же, господа! Ну, взял пестик… Н...         PV  0.833740   \n",
       "24597  – Запоздал, – ответил Красоткин. – Есть обстоя...     McDuff  0.818035   \n",
       "1354   – А, черт, да вы его в христианскую веру обрат...         PV  0.790643   \n",
       "2158   – По моему мнению… подобная прокламация… одна ...         PV  0.891061   \n",
       "\n",
       "                                                     src  \\\n",
       "20239  – Нет, нет; никогда и нигде! – вскрикнула Соня...   \n",
       "1157   Степан Аркадьич широко открыл свои блестящие, ...   \n",
       "16479  – Про кого ты говоришь… про Митю? – в недоумен...   \n",
       "3512   – Что делать, придумай, Анна, помоги. Я все пе...   \n",
       "9608   – Фатеру по ночам не нанимают; а к тому же вы ...   \n",
       "3153   – Вам-то что за дело? – спросил он вдруг с стр...   \n",
       "27553  – А простить не хочешь! – прокричал Митя Груше...   \n",
       "7942   – Да не беспокойтесь же о форме, – перебил Пор...   \n",
       "5266   – Ну, полноте, кто ж у нас на Руси себя Наполе...   \n",
       "10236  – Да что вы, Родион Романыч, такой сам не свой...   \n",
       "24260                  – Что же ты теперь хочешь делать?   \n",
       "7073          Милостивая государыня, Варвара Алексеевна!   \n",
       "6655   Осмотрев детей, они сели, уже одни, в гостиной...   \n",
       "4659   – А-зе здеся нельзя, здеся не места! – встрепе...   \n",
       "757    – Я… я спрошу его… – пробормотал Алеша. – Если...   \n",
       "5118   – Зовут водку пить. Они, верно, луга делили. Я...   \n",
       "3187   – Да помилуйте же, господа! Ну, взял пестик… Н...   \n",
       "24597  – Запоздал, – ответил Красоткин. – Есть обстоя...   \n",
       "1354   – А, черт, да вы его в христианскую веру обрат...   \n",
       "2158   – По моему мнению… подобная прокламация… одна ...   \n",
       "\n",
       "                                                     tgt  \n",
       "20239  \"No, no; never and nowhere!\" Sonya cried. \"I'l...  \n",
       "1157   Stepan Arkadyich opened his shining, clear eye...  \n",
       "16479  \"Who are you talking about . .. about Mitya?\" ...  \n",
       "3512   'What's to be done, think, Anna, help me. I've...  \n",
       "9608          and you ought to come up with the porter.\"  \n",
       "3153   \"What business is it of yours?\" he asked sudde...  \n",
       "27553  'But you do not want to forgive!' Mitya shoute...  \n",
       "7942   \"Don't worry about the form,\" Porfiry interrup...  \n",
       "5266   \"But, my goodness, who in our Russia nowadays ...  \n",
       "10236  Svidrigailov looked intently at Raskolnikov an...  \n",
       "24260                        'What are you going to do?'  \n",
       "7073                             DEAR VARVARA ALEXYEVNA,  \n",
       "6655   After looking at the children, they sat down, ...  \n",
       "4659   Achilles roused himself, his pupils widening m...  \n",
       "757    I... I'll ask him,\" Alyosha murmured. \"If it w...  \n",
       "5118   'They're inviting us to drink vodka. They've p...  \n",
       "3187   \"But for pity's sake, gentlemen! So I took the...  \n",
       "24597  'I was delayed,' Krasotkin replied. 'There wer...  \n",
       "1354   \"Ah, the devil, you'll convert him to the Chri...  \n",
       "2158   \"In my opinion... such a tract ... is nothing ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_train_df.sample(n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
