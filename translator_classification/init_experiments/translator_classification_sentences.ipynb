{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator Classification - multilingual BERT + Sentences (instead of paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "data_path = 'russian_lit_data'\n",
    "copy_path = 'copyrighted'\n",
    "noncopy_path = 'uncopyrighted'\n",
    "rus_path = 'russian'\n",
    "\n",
    "copy_files = glob.glob(data_path + '/' + copy_path + '/*.txt')\n",
    "noncopy_files = glob.glob(data_path + '/' + noncopy_path + '/*.txt')\n",
    "files = copy_files + noncopy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict: translator to books\n",
    "# NotesFromUnderground - Katz, PV, Garnett\n",
    "# PoorFolk - McDuff, Hogarth, Garnett\n",
    "\n",
    "translator_to_sents = {}\n",
    "translator_to_sents_holdout = {}\n",
    "holdout_books = ['PoorFolk', 'NotesFromUnderground']\n",
    "for f in files:\n",
    "    book_title = (f.split('/')[-1]).split('.')[0]\n",
    "    book_name = book_title.split('_')[0]\n",
    "    translator = book_title.split('_')[1]\n",
    "\n",
    "    if translator not in translator_to_sents.keys():\n",
    "        translator_to_sents[translator] = []\n",
    "        translator_to_sents_holdout[translator] = []\n",
    "\n",
    "    with open(f, \"r\") as fp:\n",
    "        book_text = fp.read()\n",
    "        book_pars = book_text.split('\\n')\n",
    "        sents = []\n",
    "\n",
    "        for par in book_pars:\n",
    "            if len(par) > 40: # and len(par) < int(490/2):\n",
    "                par_sents = nltk.tokenize.sent_tokenize(par)\n",
    "                long_sents = [s for s in par_sents if len(s) > 40]\n",
    "                if book_name in holdout_books:\n",
    "                    translator_to_sents_holdout[translator].extend(long_sents)\n",
    "                else:\n",
    "                    translator_to_sents[translator].extend(long_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "print('Train')\n",
    "for k in translator_to_sents.keys():\n",
    "    print(k, len(translator_to_sents[k]))\n",
    "    total += len(translator_to_sents[k])\n",
    "print('Total', total)\n",
    "\n",
    "total = 0\n",
    "print('\\nHoldout')\n",
    "for k in translator_to_sents_holdout.keys():\n",
    "    print(k, len(translator_to_sents_holdout[k]))\n",
    "    total += len(translator_to_sents_holdout[k])\n",
    "print('Total', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(list(translator_to_sents.keys()))\n",
    "print(le.transform([\"Garnett\", \"McDuff\", \"PV\", \"Katz\", \"Hogarth\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each translator\n",
    "# split each book into sentences with sent_tokenizer\n",
    "# drop sentences shorter than 6 words\n",
    "# add to data dict: {'idx': 0, 'label': 1, 'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\"}\n",
    "data_list = []\n",
    "i = 0\n",
    "for tr in translator_to_sents.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for s in translator_to_sents[tr]:\n",
    "        sent_dict = {'idx': i, 'labels': label, 'sentence': s}\n",
    "        data_list.append(sent_dict)\n",
    "        i += 1\n",
    "\n",
    "data_list_holdout = []\n",
    "i = 0\n",
    "for tr in translator_to_sents_holdout.keys():\n",
    "    label = le.transform([tr])[0]\n",
    "    for s in translator_to_sents_holdout[tr]:\n",
    "        sent_dict = {'idx': i, 'labels': label, 'sentence': s}\n",
    "        data_list_holdout.append(sent_dict)\n",
    "        i += 1\n",
    "        \n",
    "print(data_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df_holdout = pd.DataFrame(data_list_holdout)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "# df = shuffle(df)\n",
    "# df = df.head(100000)\n",
    "df.shape\n",
    "df_holdout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_texts = df['sentence'].values.tolist()\n",
    "# train_labels = df['labels'].values.tolist()\n",
    "test_texts = df_holdout['sentence'].values.tolist()\n",
    "test_labels = df_holdout['labels'].values.tolist()\n",
    "\n",
    "train_texts, _, train_labels, _ = train_test_split(\n",
    "    df['sentence'].values.tolist(), df['labels'].values.tolist(),\n",
    "    stratify = df['labels'], train_size=0.55, shuffle=True\n",
    ")\n",
    "\n",
    "print('train size: ', len(list(train_labels)))\n",
    "print('test size: ', len(list(test_labels)))\n",
    "\n",
    "sentences = {}\n",
    "sentences['train'] = []\n",
    "sentences['test'] = []\n",
    "for t, l in zip(train_texts, train_labels):\n",
    "    datum = {'label': l, 'text': t}\n",
    "    sentences['train'].append(datum)\n",
    "for t, l in zip(test_texts, test_labels):\n",
    "    datum = {'label': l, 'text': t}\n",
    "    sentences['test'].append(datum)\n",
    "\n",
    "print(sentences['train'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = \"bert-base-multilingual-cased\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "train_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=sentences['train']))\n",
    "test_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=sentences['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = translator_to_sents.keys()\n",
    "id_list = le.transform(list(label_list))\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for l, i in zip(label_list, id_list):\n",
    "    id2label[i] = l\n",
    "    label2id[l] = i\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# device (turn on GPU acceleration for faster execution)\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "if fine_tune:\n",
    "    # model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels = len(translator_to_sents.keys()))\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "epochs = 10\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "run = wandb.init(\n",
    "        # Set the project where this run will be logged\n",
    "        project=\"translator-classification\",\n",
    "        # Track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"epochs\": epochs,\n",
    "        },\n",
    "    )\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"]=\"translator-classification\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"]=\"translator_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/trunk/kkatsy/classification_10epochs_holdout\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"wandb\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune:\n",
    "    trainer.evaluate()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_tuned = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_tuned:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"/trunk/kkatsy/classification_10epochs_holdout/checkpoint-10512\")\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_tuned:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/trunk/kkatsy/classification_10epochs_holdout\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"wandb\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, metrics = trainer.predict(tokenized_test, metric_key_prefix=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translators = [id2label[l] for l in preds]\n",
    "\n",
    "pred_count = {}\n",
    "for i in label2id.keys():\n",
    "    count = translators.count(i)\n",
    "    pred_count[i] = count\n",
    "\n",
    "pred_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "courses = list(pred_count.keys())\n",
    "values = list(pred_count.values())\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(courses, values, width = 0.4)\n",
    "plt.title(\"Predicted Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_count = {}\n",
    "for i in id2label.keys():\n",
    "    count = test_labels.count(i)\n",
    "    true_count[id2label[i]] = count\n",
    "\n",
    "true_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = list(true_count.keys())\n",
    "values = list(true_count.values())\n",
    "  \n",
    "fig2 = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(courses, values, width = 0.4)\n",
    "plt.title(\"Actual Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(labels, preds, normalize='true')\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels=label2id)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
